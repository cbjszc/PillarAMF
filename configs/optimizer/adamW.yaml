# @package optimizer
_target_: torch.optim.AdamW
betas: [0.9, 0.99]  # 使用默认 betas
weight_decay: 0.01  # 保持权重衰减
amsgrad: False  # 保持 amsgrad 设置不变